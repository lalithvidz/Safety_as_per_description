{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os;\n",
    "import tensorflow as tf\n",
    "os.environ['PYTHONHASHSEED']=str(7)\n",
    "\n",
    "# Reproduce the results\n",
    "def reset_random_seeds():\n",
    "   os.environ['PYTHONHASHSEED']=str(7)\n",
    "   #np.random.seed(7)\n",
    "   #random.seed(7)\n",
    "   tf.random.set_seed(7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import tokenize, stem\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import string\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "\", \".join(stopwords.words('english'))\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from tensorflow.keras.layers import Flatten, Activation, Dense, LSTM, BatchNormalization, Embedding, Dropout, Flatten, Bidirectional, GlobalMaxPool1D\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, recall_score, precision_score, roc_auc_score\n",
    "from tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Flatten\n",
    "\n",
    "\n",
    "import pickle as pkl\n",
    "from tensorflow.keras.models import Model, Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size: 3118\n",
      "(340, 100) (340, 5) (85, 100) (85, 5)\n",
      "Epoch 1/10\n",
      "43/43 [==============================] - 6s 93ms/step - loss: 1.5199 - accuracy: 0.3666 - val_loss: 0.9900 - val_accuracy: 0.7412\n",
      "Epoch 2/10\n",
      "43/43 [==============================] - 4s 86ms/step - loss: 1.0926 - accuracy: 0.7442 - val_loss: 0.9432 - val_accuracy: 0.7412\n",
      "Epoch 3/10\n",
      "43/43 [==============================] - 4s 87ms/step - loss: 1.0008 - accuracy: 0.7460 - val_loss: 0.9424 - val_accuracy: 0.7412\n",
      "Epoch 4/10\n",
      "43/43 [==============================] - 4s 87ms/step - loss: 1.0376 - accuracy: 0.7441 - val_loss: 0.9476 - val_accuracy: 0.7412\n",
      "Epoch 5/10\n",
      "43/43 [==============================] - 4s 86ms/step - loss: 1.0724 - accuracy: 0.7291 - val_loss: 0.9430 - val_accuracy: 0.7412\n",
      "Epoch 6/10\n",
      "43/43 [==============================] - 4s 86ms/step - loss: 0.9268 - accuracy: 0.7666 - val_loss: 0.9414 - val_accuracy: 0.7412\n",
      "Epoch 7/10\n",
      "43/43 [==============================] - 4s 88ms/step - loss: 1.0116 - accuracy: 0.7346 - val_loss: 0.9328 - val_accuracy: 0.7412\n",
      "Epoch 8/10\n",
      "43/43 [==============================] - 4s 90ms/step - loss: 0.9872 - accuracy: 0.7294 - val_loss: 0.9323 - val_accuracy: 0.7412\n",
      "Epoch 9/10\n",
      "43/43 [==============================] - 4s 86ms/step - loss: 0.9483 - accuracy: 0.7454 - val_loss: 0.9261 - val_accuracy: 0.7412\n",
      "Epoch 10/10\n",
      "43/43 [==============================] - 4s 89ms/step - loss: 0.9497 - accuracy: 0.7483 - val_loss: 0.9248 - val_accuracy: 0.7412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Admin\\Anaconda3\\lib\\tkinter\\__init__.py\", line 1705, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"<ipython-input-8-4488ced977a4>\", line 260, in predict_chat\n",
      "    result = model.predict(sentence)\n",
      "  File \"C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 1629, in predict\n",
      "    tmp_batch_outputs = self.predict_function(iterator)\n",
      "  File \"C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 828, in __call__\n",
      "    result = self._call(*args, **kwds)\n",
      "  File \"C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 871, in _call\n",
      "    self._initialize(args, kwds, add_initializers_to=initializers)\n",
      "  File \"C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 726, in _initialize\n",
      "    *args, **kwds))\n",
      "  File \"C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 2969, in _get_concrete_function_internal_garbage_collected\n",
      "    graph_function, _ = self._maybe_define_function(args, kwargs)\n",
      "  File \"C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 3361, in _maybe_define_function\n",
      "    graph_function = self._create_graph_function(args, kwargs)\n",
      "  File \"C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 3206, in _create_graph_function\n",
      "    capture_by_value=self._capture_by_value),\n",
      "  File \"C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\", line 990, in func_graph_from_py_func\n",
      "    func_outputs = python_func(*func_args, **func_kwargs)\n",
      "  File \"C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 634, in wrapped_fn\n",
      "    out = weak_wrapped_fn().__wrapped__(*args, **kwds)\n",
      "  File \"C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\", line 977, in wrapper\n",
      "    raise e.ag_error_metadata.to_exception(e)\n",
      "ValueError: in user code:\n",
      "\n",
      "    C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1478 predict_function  *\n",
      "        return step_function(self, iterator)\n",
      "    C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1468 step_function  **\n",
      "        outputs = model.distribute_strategy.run(run_step, args=(data,))\n",
      "    C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1259 run\n",
      "        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n",
      "    C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2730 call_for_each_replica\n",
      "        return self._call_for_each_replica(fn, args, kwargs)\n",
      "    C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3417 _call_for_each_replica\n",
      "        return fn(*args, **kwargs)\n",
      "    C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1461 run_step  **\n",
      "        outputs = model.predict_step(data)\n",
      "    C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1434 predict_step\n",
      "        return self(x, training=False)\n",
      "    C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:998 __call__\n",
      "        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n",
      "    C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:274 assert_input_compatibility\n",
      "        ', found shape=' + display_shape(x.shape))\n",
      "\n",
      "    ValueError: Input 0 is incompatible with layer model_14: expected shape=(None, 100), found shape=(None, 25)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import ttk\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "#STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "win = tk.Tk()\n",
    "win.title('Industrial Safety NLP based Chatbot')  # Window Title\n",
    "\n",
    "# Step 1: Import data frame name ---------------------------------\n",
    "\n",
    "Name=ttk.Label(win,text=\"File Name\")\n",
    "Name.grid(row=0,column=0,sticky=tk.W)\n",
    "\n",
    "Name_var=tk.StringVar()\n",
    "Name_entrybox=ttk.Entry(win,width=16,textvariable=Name_var)\n",
    "Name_entrybox.grid(row=0,column=1)\n",
    "\n",
    "def Import_Data():\n",
    "    global DB\n",
    "    DF_Name=Name_var.get()\n",
    "    if len(DF_Name)==0:\n",
    "        confirm=\"File Not Present\"\n",
    "    DB_extension=re.findall(\"\\..*\", DF_Name) \n",
    "    if DB_extension==['.xlsx']:\n",
    "        DB=pd.read_excel(DF_Name)\n",
    "        confirm=\"Done\"\n",
    "    elif DB_extension==['.csv']:\n",
    "        DB=pd.read_csv(DF_Name)\n",
    "        confirm=\"Done\"\n",
    "    Confirm_entrybox=ttk.Entry(win,width=16)\n",
    "    Confirm_entrybox.grid(row=0,column=3)\n",
    "    Confirm_entrybox.insert(1,str(confirm))   \n",
    "\n",
    "Import_Data_Button=ttk.Button(win,text=\"Import Data\",command=Import_Data)\n",
    "Import_Data_Button.grid(row=0,column=2)\n",
    "\n",
    "\n",
    "# Step 2: Target data frame name ---------------------------------\n",
    "\n",
    "Target=ttk.Label(win,text=\"Target Colummn\")\n",
    "Target.grid(row=1,column=0,sticky=tk.W)\n",
    "\n",
    "Target_var=tk.StringVar()\n",
    "Target_entrybox=ttk.Entry(win,width=16,textvariable=Target_var)\n",
    "Target_entrybox.grid(row=1,column=1)\n",
    "\n",
    "def Target_Data():\n",
    "    global DB, Target_Name\n",
    "    Target_Name=Target_var.get()\n",
    "    \n",
    "    Column_name = DB.columns\n",
    "    Column_name\n",
    "    found=0\n",
    "\n",
    "    for i in range(len(Column_name)):\n",
    "        if Column_name[i]==Target_Name:\n",
    "            confirm=\"Found\"\n",
    "        else:\n",
    "            confirm=\"Not Found\"\n",
    "    \n",
    "    Confirm_entrybox=ttk.Entry(win,width=16)\n",
    "    Confirm_entrybox.grid(row=1,column=3)\n",
    "    Confirm_entrybox.insert(1,str(confirm))\n",
    "\n",
    "\n",
    "Target_Button=ttk.Button(win,text=\"Import Target\",command=Target_Data)\n",
    "Target_Button.grid(row=1,column=2)\n",
    "\n",
    "## Step 3: pre-processing  ---------------------------------\n",
    "\n",
    "process=ttk.Label(win,text=\"Pre-processing\")\n",
    "process.grid(row=2,column=0,sticky=tk.W)\n",
    "\n",
    "global text\n",
    "\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    PUNCT_TO_REMOVE = string.punctuation\n",
    "    return text.translate(str.maketrans('','',PUNCT_TO_REMOVE))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    STOPWORDS = set(stopwords.words('english'))\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "def clean_text(text):\n",
    "    text = text.strip()\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "def preprocess():\n",
    "    global DB\n",
    "    dict1 = {'I':1, 'II':2,'III':3,'IV':4, 'V':5}\n",
    "    DB['Accident Level'] = DB['Accident Level'].map(dict1)\n",
    "    \n",
    "    \n",
    "    text = DB['Description']\n",
    "    text= text.str.lower()                                 \n",
    "    text = text.apply(lambda text: remove_punctuation(text))\n",
    "    text = text.apply(lambda text : remove_stopwords(text))    \n",
    "    DB['Description'] = DB['Description'].map(lambda text : clean_text(text))\n",
    "    DB['Description'].replace('', np.nan, inplace = True)\n",
    "    DB = DB.dropna()\n",
    "    \n",
    "    \n",
    "    DB= DB.drop(['Data','Unnamed: 0'],axis=1)\n",
    "    confirm=\"Done\"\n",
    "    Confirm_entrybox=ttk.Entry(win,width=20)\n",
    "    Confirm_entrybox.grid(row=2,column=3)\n",
    "    Confirm_entrybox.insert(1,str(confirm))\n",
    "    \n",
    "    return DB\n",
    "\n",
    "process_button=ttk.Button(win,text=\"Pre-process\",command=preprocess)\n",
    "process_button.grid(row=2,column=2)\n",
    "\n",
    "\n",
    "def Embed():\n",
    "    global DB, x_new1, y_new1, embedding_size, embedding_matrix, num_words,maxlen,vocab_size,X_train, X_test, y_train, y_test\n",
    "    \n",
    "    x_new1 = DB['Description']\n",
    "    y_new1 = DB['Accident Level']\n",
    "    \n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(x_new1, y_new1, test_size = 0.20, random_state = 1, stratify = y_new1)\n",
    "    \n",
    "    y_train = pd.get_dummies(y_train).values\n",
    "    y_test = pd.get_dummies(y_test).values\n",
    "    #y_train = to_categorical(y_train)\n",
    "    #y_test = to_categorical(y_test)\n",
    "    tokenizer = Tokenizer(num_words=5000)\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "    X_train = tokenizer.texts_to_sequences(X_train)\n",
    "    X_test = tokenizer.texts_to_sequences(X_test)\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    print(\"vocab_size:\", vocab_size)\n",
    "\n",
    "    maxlen = 100\n",
    "\n",
    "    X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "    X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
    "\n",
    "    embedding_size = 200\n",
    "    embeddings_dictionary = dict()\n",
    "\n",
    "    glove_file = open('glove.6B.200d.txt', encoding=\"utf8\")\n",
    "\n",
    "    for line in glove_file:\n",
    "        records = line.split()\n",
    "        word = records[0]\n",
    "        vector_dimensions = np.asarray(records[1:], dtype='float32')\n",
    "        embeddings_dictionary[word] = vector_dimensions\n",
    "\n",
    "    glove_file.close()\n",
    "\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_size))\n",
    "\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        embedding_vector = embeddings_dictionary.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector\n",
    "            \n",
    "    confirm=\"Done\"        \n",
    "    Confirm_entrybox=ttk.Entry(win,width=20)\n",
    "    Confirm_entrybox.grid(row=3,column=3)\n",
    "    Confirm_entrybox.insert(1,str(confirm))\n",
    "    \n",
    "    \n",
    "embed_button=ttk.Button(win,text=\"Embedding\",command=Embed)\n",
    "embed_button.grid(row=3,column=2)\n",
    "\n",
    "model1=ttk.Label(win,text=\"Model predict\")\n",
    "model1.grid(row=4,column=0,sticky=tk.W)   \n",
    "\n",
    "def model_best():    \n",
    "    global maxlen,vocab_size,embedding_size,embedding_matrix,X_train, y_train,X_test, y_test, model\n",
    "    reset_random_seeds()\n",
    "    \n",
    "    # Build a LSTM Neural Network\n",
    "    deep_inputs = Input(shape=(maxlen,))\n",
    "    embedding_layer = Embedding(vocab_size, embedding_size, weights=[embedding_matrix], trainable=False)(deep_inputs)\n",
    "    LSTM_Layer_1 = Bidirectional(LSTM(128, return_sequences = True))(embedding_layer)\n",
    "    max_pool_layer_1 = GlobalMaxPool1D()(LSTM_Layer_1)\n",
    "    drop_out_layer_1 = Dropout(0.5, input_shape = (256,))(max_pool_layer_1)\n",
    "    dense_layer_1 = Dense(128, activation = 'relu')(drop_out_layer_1)\n",
    "    drop_out_layer_2 = Dropout(0.5, input_shape = (128,))(dense_layer_1)\n",
    "    dense_layer_2 = Dense(64, activation = 'relu')(drop_out_layer_2)\n",
    "    drop_out_layer_3 = Dropout(0.5, input_shape = (64,))(dense_layer_2)\n",
    "\n",
    "    dense_layer_3 = Dense(32, activation = 'relu')(drop_out_layer_3)\n",
    "    drop_out_layer_4 = Dropout(0.5, input_shape = (32,))(dense_layer_3)\n",
    "\n",
    "    dense_layer_4 = Dense(10, activation = 'relu')(drop_out_layer_4)\n",
    "    drop_out_layer_5 = Dropout(0.5, input_shape = (10,))(dense_layer_4)\n",
    "\n",
    "    dense_layer_5 = Dense(5, activation='softmax')(drop_out_layer_5)\n",
    "    dense_layer_3 = Dense(5, activation='softmax')(drop_out_layer_3)\n",
    "\n",
    "    LSTM_Layer_1 = LSTM(128)(embedding_layer)\n",
    "    dense_layer_1 = Dense(5, activation='softmax')(LSTM_Layer_1)\n",
    "    \n",
    "    model = Model(inputs=deep_inputs, outputs=dense_layer_1)\n",
    "    model = Model(inputs=deep_inputs, outputs=dense_layer_5)\n",
    "    model = Model(inputs=deep_inputs, outputs=dense_layer_3)\n",
    "\n",
    "    opt = SGD(lr=0.001, momentum=0.9)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    # fit the keras model on the dataset\n",
    "    print(X_train.shape, y_train.shape,X_test.shape, y_test.shape)\n",
    "    training_history = model.fit(X_train, y_train, epochs=10, batch_size=8, verbose=1, validation_data=(X_test, y_test))\n",
    "    training_history_DB = pd.DataFrame(training_history.history)\n",
    "    \n",
    "    accuracy_field = ttk.Entry(win, width=20)\n",
    "    accuracy_field.grid(row=6, column=2)\n",
    "    accuracy_field.insert(1, str(training_history.history['accuracy'][-1]))\n",
    "    \n",
    "    val_accuracy_field = ttk.Entry(win, width=20)\n",
    "    val_accuracy_field.grid(row=6, column=3)\n",
    "    val_accuracy_field.insert(1, str(training_history.history['val_accuracy'][-1]))\n",
    "\n",
    "final_button = ttk.Button(win, text = \"Training\", command = model_best)  \n",
    "final_button.grid(row=6, column=1)\n",
    "\n",
    "def pkl():\n",
    "    global model\n",
    "    model.save('BiLSTM_model123.h5')\n",
    "    model.save_weights('BiLSTM_model_weights.h5')\n",
    "    confirm = \"model is saved\"\n",
    "    model_accuracy = ttk.Entry(win, width=20)\n",
    "    model_accuracy.grid(row=7, column=2)\n",
    "    model_accuracy.insert(1, str(confirm))\n",
    "    \n",
    "final_pickle = ttk.Button(win, text=\"Pickle\", command=pkl)\n",
    "final_pickle.grid(row=7, column=1)\n",
    "\n",
    "\n",
    "Description_var = tk.StringVar()\n",
    "Description_entrybox = ttk.Entry(win,width=20,textvariable=Description_var)\n",
    "Description_entrybox.grid(row=9,column=0) \n",
    "\n",
    "\n",
    "def predict_chat():\n",
    "    global sentence,X_train\n",
    "    \n",
    "   \n",
    "    sentence = Description_var.get()\n",
    "    sentence = sentence.lower()                                 \n",
    "    sentence = remove_punctuation(sentence)\n",
    "    sentence =  remove_stopwords(sentence)    \n",
    "    sentence =  clean_text(sentence)\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words=5000)\n",
    "    #tokenizer.fit_on_texts(X_train)\n",
    "    sentence = tokenizer.texts_to_sequences(sentence)\n",
    "    sentence = pad_sequences(sentence, padding='post', maxlen=25)\n",
    "    \n",
    "    result = model.predict(sentence)\n",
    "    predicted_index = np.argmax(result[0])\n",
    "    mapping = {1:'Level 1', 2:'Level 2', 3:'Level 3'}\n",
    "    predicted = mapping[predicted_index]\n",
    "    predicted\n",
    "    \n",
    "    confirm = \"The level is : \"+ predicted\n",
    "    confirm_pred = ttk.Entry(win, width=20)\n",
    "    confirm_pred.grid(row=8, column=2)\n",
    "    confirm_pred.insert(1, str(confirm))\n",
    "    \n",
    "predicted_button = ttk.Button(win, text=\"Output\", command = predict_chat)\n",
    "predicted_button.grid(rows=8,column=1)\n",
    "    \n",
    "win.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
